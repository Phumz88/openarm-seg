{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "print(\"imported\")\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "sys.path.append('src/')\n",
    "import nn\n",
    "import process_data\n",
    "import nibabel as nib\n",
    "# import cv2\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.sparse\n",
    "from scipy.misc import imrotate, imresize\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.ndimage import rotate\n",
    "from skimage import exposure\n",
    "from skimage.io import imread, imsave\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "print(local_device_protos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(L, class_labels):\n",
    "    \"\"\"\n",
    "    2D array (image) of segmentation labels -> .npy\n",
    "    # One Hot Encode the label 2d array -> .npy files with dim (h, w, len(class_labels))\n",
    "    # num classes will be 8? but currently dynamically allocated based on num colors in all scans.\n",
    "    \"\"\"\n",
    "    h, w = L.shape  # Should be 482, 395 (unless resized)\n",
    "    try:\n",
    "        encoded = np.array([list(map(class_labels.index, L.flatten()))])\n",
    "\n",
    "        L = encoded.reshape(h, w)\n",
    "\n",
    "        Lhot = np.zeros((L.shape[0], L.shape[1], len(class_labels)))\n",
    "        for i in range(L.shape[0]):\n",
    "            for j in range(L.shape[1]):\n",
    "                Lhot[i,j,L[i,j]] = 1\n",
    "        return Lhot  # Should be shape (482, 395, 9)\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "def uncode_one_hot(npy_file):\n",
    "    \"\"\"\n",
    "    .npy file -> JPEG\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def show_images(images, cols = 1, titles = None):\n",
    "    \"\"\"Display a list of images in a single figure with matplotlib.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "    cols (Default = 1): Number of columns in figure (number of rows is \n",
    "                        set to np.ceil(n_images/float(cols))).\n",
    "    \n",
    "    titles: List of titles corresponding to each image. Must have\n",
    "            the same length as titles.\n",
    "    \"\"\"\n",
    "    assert((titles is None)or (len(images) == len(titles)))\n",
    "    n_images = len(images)\n",
    "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "    fig = plt.figure()\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
    "#         if image.ndim == 2:\n",
    "#             plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "    plt.show()\n",
    "    \n",
    "def load_sparse_csr(filename):\n",
    "    # Sparse matrix reading function to read our raw .npz files\n",
    "    assert filename.endswith('.npz')\n",
    "    loader = np.load(filename)  # filename must end with .npz\n",
    "    return scipy.sparse.csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])\n",
    "\n",
    "def get_raw_pixel_classes():\n",
    "    #import nibabel as nib\n",
    "    base_data_dir = \"/home/jessica/Documents/hart-seg-ml/allrawnifti\"\n",
    "    example_segmentation = os.path.join(base_data_dir, 'trial8_30_fs_seg_ak5_lh4_TRANS.nii')\n",
    "    scan_voxel = nib.load(example_segmentation)\n",
    "    struct_arr = scan_voxel.get_data()\n",
    "    n, h, w = struct_arr.shape\n",
    "    class_labels = list(np.unique(struct_arr))\n",
    "    \n",
    "def check_one_hot(encoded_img):\n",
    "    print(encoded_img.shape)\n",
    "    return np.all(np.sum(encoded_img, axis=2) == 1.)\n",
    "\n",
    "def batch_img_resize(images, h = 256, w = 256):\n",
    "    images_resized = np.zeros([0, newHeight, newWidth], dtype=np.uint8)\n",
    "    for  image in range(images.shape[0]):\n",
    "        temp = imresize(images[image], [h, w], 'nearest')\n",
    "        images_resized = np.append(images_resized, np.expand_dims(temp, axis=0), axis=0)\n",
    "    return images_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pixel_classes =[0, 7, 8, 9, 45, 51, 52, 53, 68]  # Expected raw grayscale values for each pixel\n",
    "#directory = \"/Users/nozik/Documents/HARTresearch/allpreprocessed\"\n",
    "directory = \"/home/jessica/Documents/hart-seg-ml/30_deg_training\"\n",
    "filenames = []  # Stores all filenames\n",
    "raw_images = []  # Stores X (Raw cross section images as 2D np.ndarray)\n",
    "segmentations = []  # Stores Y (Labeled/Segmented image as one-hot-encoded NumClasses-D np.ndarray)\n",
    "h, w = 512, 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir(directory):\n",
    "    class_labels = set()\n",
    "    if not folder.startswith('.'):\n",
    "        path = os.path.join(directory, folder)\n",
    "        print(path)\n",
    "        files = sorted([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and not f.startswith('.')])\n",
    "        \n",
    "        # Class label sanity check\n",
    "#         for f in files:\n",
    "#             if 'label' in f:\n",
    "#                 img = imread(os.path.join(path, f), flatten=True)\n",
    "#                 class_labels = class_labels.union(np.unique(img))\n",
    "#         if not class_labels.issubset(raw_pixel_classes):\n",
    "#             print(\"Class labels found in labeled images do not match the expected classes for scan {}\".format(folder))\n",
    "#             print(\"Expected {}\".format(raw_pixel_classes))\n",
    "#             print(\"Received {}\".format(sorted(class_labels)))\n",
    "#             break\n",
    "        \n",
    "        # Sanity image read and show some images in pairs (play with the range inputs)\n",
    "#         for f in range(0, len(files), 2):\n",
    "#             label_name = files[f]\n",
    "#             raw_name = files[f+1]\n",
    "#             label_img = imread(os.path.join(path, label_name), flatten=True)\n",
    "#             raw_img = load_sparse_csr(os.path.join(path, raw_name)).toarray()  # Load sparse csr mat img -> to 2D numpy array\n",
    "#             show_images([label_img, raw_img], titles=[label_name, raw_name])\n",
    "        \n",
    "        # Set up Datasets (X, Y) pairs of data ->\n",
    "        # files are sorted by the name: either '#_label' or '#_raw'\n",
    "        for f in files:\n",
    "            print(f, end=' ')\n",
    "            if 'label' in f:\n",
    "                img = imread(os.path.join(path, f), flatten=True)\n",
    "            else:\n",
    "                img = load_sparse_csr(os.path.join(path, f)).toarray()            \n",
    "            \n",
    "#             imresize(seg[:,:,1],(h,w), interp='nearest')/255.0\n",
    "            npad = ((15, 15), (58, 59))  # Pads to size 512, 512\n",
    "            img = np.pad(img, pad_width=npad, mode='constant', constant_values=0)\n",
    "            if 'raw' in f:\n",
    "                raw_images.append(img)\n",
    "            elif 'label' in f:\n",
    "                encoded_img = one_hot_encode(img, raw_pixel_classes)\n",
    "                segmentations.append(encoded_img)\n",
    "            filenames.append(os.path.join(folder, f))\n",
    "    print(\"\")\n",
    "            \n",
    "\n",
    "# print(filenames)\n",
    "\n",
    "            \n",
    "# images = np.array(images)\n",
    "# segmentations = np.round(np.array(segmentations)).astype('uint8')\n",
    "\n",
    "\n",
    "# study_num = int(2)\n",
    "# train_lst = np.load('data/splits/train_lst_' + str(study_num) + '.npy')\n",
    "# val_lst = np.load('data/splits/val_lst_' + str(study_num) + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# raw_90_deg_training = np.array(raw_images)\n",
    "# seg_90_deg_training = np.array(segmentations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Training, Cross Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(raw_images), len(segmentations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Same Scan cannot be used across Train, Validation and Test sets\n",
    "TODO: Different weight conditions and angles may be used to segment other raw_scans\n",
    "TODO: Bounding Box, image resizing, padding edges\n",
    "\"\"\"\n",
    "# raw_images holds our X data\n",
    "# segmentations holds out Y data\n",
    "x_train, y_train = [], []\n",
    "x_val, y_val = [], []\n",
    "x_test, y_test = [], []\n",
    "\n",
    "percent_train, percent_val, percent_test = 60, 10, 30\n",
    "num_train = np.round(len(raw_images) * percent_train/100).astype(np.int)\n",
    "num_val = np.round(num_train + len(raw_images) * percent_val/100).astype(np.int)\n",
    "num_test = np.round(num_val + len(raw_images) * percent_test/100).astype(np.int)\n",
    "\n",
    "print(\"num_train: \", num_train, \"num_val: \", num_val, \"num_test: \", num_test)\n",
    "\n",
    "assert len(raw_images) == len(segmentations)\n",
    "rand_indices = list(np.random.choice(len(raw_images), len(raw_images), replace=False))\n",
    "\n",
    "for i in rand_indices[:num_train]:\n",
    "    x_train.append(raw_images[i])\n",
    "    y_train.append(segmentations[i])\n",
    "for j in rand_indices[num_train:num_val]:\n",
    "    x_val.append(raw_images[j])\n",
    "    y_val.append(segmentations[j])\n",
    "for k in rand_indices[num_val:num_test]:\n",
    "    x_test.append(raw_images[k])\n",
    "    y_test.append(segmentations[k])\n",
    "\n",
    "        \n",
    "x_train = np.array(x_train).reshape((len(x_train), h, w, 1))\n",
    "x_test = np.array(x_test).reshape((len(x_test), h, w, 1))\n",
    "x_val = np.array(x_val).reshape((len(x_val), h, w, 1))\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "# Fix data padding to create square 482 by 482 matrix\n",
    "# npad = ((0, 0), (15, 15), (58, 59), (0, 0))\n",
    "# x_train = np.pad(x_train, pad_width=npad, mode='constant', constant_values=0)\n",
    "# x_test = np.pad(x_test, pad_width=npad, mode='constant', constant_values=0)\n",
    "# y_train = np.pad(y_train, pad_width=npad, mode='constant', constant_values=0)\n",
    "# y_test = np.pad(y_test, pad_width=npad, mode='constant', constant_values=0)\n",
    "\n",
    "# print()\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)\n",
    "\n",
    "\n",
    "#     image = imresize(imread(directory + folder + '/' + folder + '.jpg', flatten = True),(h, w))\n",
    "#     images.append(image)\n",
    "#     filenames.append(folder)\n",
    "#     seg = np.load(directory+folder+'/seg.npy')\n",
    "#     temp = np.zeros((h,w,1))\n",
    "#     temp[:,:,1] = imresize(seg[:,:,1],(h,w), interp='nearest')/255.0\n",
    "#     segmentations.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Unet(object):        \n",
    "    def __init__(self, mean, weight_decay, learning_rate, label_dim = 8, dropout = 0.9):\n",
    "        self.x_train = tf.placeholder(tf.float32, [None, h, w, 1])\n",
    "        self.y_train = tf.placeholder(tf.float32, [None, h, w, 9])\n",
    "        self.x_test = tf.placeholder(tf.float32, [None, h, w, 1])\n",
    "        self.y_test = tf.placeholder(tf.float32, [None, h, w, 9])\n",
    "        \n",
    "        self.label_dim = label_dim\n",
    "        self.weight_decay = weight_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.output = self.unet(self.x_train, mean, keep_prob=self.dropout)\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.output, labels = self.y_train))\n",
    "        self.opt = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        self.pred = self.unet(self.x_test, mean, reuse = True, keep_prob = 1.0)\n",
    "        self.loss_summary = tf.summary.scalar('loss', self.loss)\n",
    "    \n",
    "    # Gradient Descent on mini-batch\n",
    "    def fit_batch(self, sess, x_train, y_train):\n",
    "        _, loss, loss_summary = sess.run((self.opt, self.loss, self.loss_summary), feed_dict={self.x_train: x_train, self.y_train: y_train})\n",
    "        return loss, loss_summary\n",
    "    \n",
    "    def predict(self, sess, x):\n",
    "        prediction = sess.run((self.pred), feed_dict={self.x_test: x})\n",
    "        return prediction\n",
    "\n",
    "    def unet(self, input, mean, keep_prob = 0.9, reuse = None):\n",
    "        with tf.variable_scope('vgg', reuse=reuse):\n",
    "            input = input - mean  # Demean\n",
    "            \n",
    "            pool_ = lambda x: nn.max_pool(x, 2, 2)\n",
    "            conv_ = lambda x, output_depth, name, padding = 'SAME', relu = True, filter_size = 3: nn.conv(x, filter_size, output_depth, 1, self.weight_decay, \n",
    "                                                                                                           name=name, padding=padding, relu=relu)\n",
    "            deconv_ = lambda x, output_depth, name: nn.deconv(x, 2, output_depth, 2, self.weight_decay, name=name)\n",
    "            \n",
    "            conv_1_1 = conv_(input, 64, 'conv1_1')\n",
    "            conv_1_2 = conv_(conv_1_1, 64, 'conv1_2')\n",
    "\n",
    "            pool_1 = pool_(conv_1_2)\n",
    "\n",
    "            conv_2_1 = conv_(pool_1, 128, 'conv2_1')\n",
    "            conv_2_2 = conv_(conv_2_1, 128, 'conv2_2')\n",
    "\n",
    "            pool_2 = pool_(conv_2_2)\n",
    "\n",
    "            conv_3_1 = conv_(pool_2, 256, 'conv3_1')\n",
    "            conv_3_2 = conv_(conv_3_1, 256, 'conv3_2')\n",
    "\n",
    "            pool_3 = pool_(conv_3_2)\n",
    "\n",
    "            conv_4_1 = conv_(pool_3, 512, 'conv4_1')\n",
    "            conv_4_2 = conv_(conv_4_1, 512, 'conv4_2')\n",
    "\n",
    "            pool_4 = pool_(conv_4_2)\n",
    "\n",
    "            conv_5_1 = conv_(pool_4, 1024, 'conv5_1')\n",
    "            conv_5_2 = conv_(conv_5_1, 1024, 'conv5_2')\n",
    "            \n",
    "            pool_5 = pool_(conv_5_2)\n",
    "            \n",
    "            conv_6_1 = tf.nn.dropout(conv_(pool_5, 2048, 'conv6_1'), keep_prob)\n",
    "            conv_6_2 = tf.nn.dropout(conv_(conv_6_1, 2048, 'conv6_2'), keep_prob)\n",
    "            \n",
    "            up_7 = tf.concat([deconv_(conv_6_2, 1024, 'up7'), conv_5_2], 3)  # Error here rn\n",
    "            \n",
    "            conv_7_1 = conv_(up_7, 1024, 'conv7_1')\n",
    "            conv_7_2 = conv_(conv_7_1, 1024, 'conv7_2')\n",
    "\n",
    "            up_8 = tf.concat([deconv_(conv_7_2, 512, 'up8'), conv_4_2], 3)\n",
    "            \n",
    "            conv_8_1 = conv_(up_8, 512, 'conv8_1')\n",
    "            conv_8_2 = conv_(conv_8_1, 512, 'conv8_2')\n",
    "\n",
    "            up_9 = tf.concat([deconv_(conv_8_2, 256, 'up9'), conv_3_2], 3)\n",
    "            \n",
    "            conv_9_1 = conv_(up_9, 256, 'conv9_1')\n",
    "            conv_9_2 = conv_(conv_9_1, 256, 'conv9_2')\n",
    "\n",
    "            up_10 = tf.concat([deconv_(conv_9_2, 128, 'up10'), conv_2_2], 3)\n",
    "            \n",
    "            conv_10_1 = conv_(up_10, 128, 'conv10_1')\n",
    "            conv_10_2 = conv_(conv_10_1, 128, 'conv10_2')\n",
    "\n",
    "            up_11 = tf.concat([deconv_(conv_10_2, 64, 'up11'), conv_1_2], 3)\n",
    "            \n",
    "            conv_11_1 = conv_(up_11, 64, 'conv11_1')\n",
    "            conv_11_2 = conv_(conv_11_1, 64, 'conv11_2')\n",
    "            \n",
    "            conv_12 = conv_(conv_11_2, 9, 'conv12_2', filter_size = 1, relu = False)\n",
    "            return conv_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "mean = 0\n",
    "weight_decay = 1e-6\n",
    "learning_rate = 1e-4\n",
    "label_dim = 8\n",
    "maxout = False\n",
    "\n",
    "# Create TF graph and initialize variables\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "model = Unet(mean, weight_decay, learning_rate, label_dim , dropout = 0.5)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore old model\n",
    "# saver = tf.train.Saver()\n",
    "# saver.restore(sess, '/media/deoraid03/jeff/models/a4c_experiments/deep_256_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "nn.train(sess, model, x_train, y_train, x_val, y_val, epochs = 1000, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# IOU Accuracies for each label\n",
    "print(nn.validate(sess, model, x_val, y_val))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "models_dir = '/home/jessica/Documents/hart-seg-ml/models/'\n",
    "model_name = '30_deg_training_sorted_inputs'\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(os.path.join(models_dir, model_name), model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "saver = tf.train.import_meta_graph('/home/jessica/Documents/hart-seg-ml/models/test_june18/test_june18.meta')\n",
    "saver.restore(sess, tf.train.latest_checkpoint('./models/test_june18'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/jessica/Documents/hart-seg-ml/test\"\n",
    "filenames = []  # Stores all filenames\n",
    "raw_images = []  # Stores X (Raw cross section images as 2D np.ndarray)\n",
    "segmentations = []  # Stores Y (Labeled/Segmented image as one-hot-encoded NumClasses-D np.ndarray)\n",
    "\n",
    "for folder in os.listdir(directory):\n",
    "    if not folder.startswith('.'):\n",
    "        path = os.path.join(directory, folder)\n",
    "        files = sorted([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and not f.startswith('.')])\n",
    "        \n",
    "        for f in files:\n",
    "            print(f, end=' ')\n",
    "            if 'label' in f:\n",
    "                img = imread(os.path.join(path, f), flatten=False)\n",
    "            else:\n",
    "                img = load_sparse_csr(os.path.join(path, f)).toarray()            \n",
    "            \n",
    "#             imresize(seg[:,:,1],(h,w), interp='nearest')/255.0\n",
    "            npad = ((15, 15), (58, 59))  # Pads to size 512, 512\n",
    "            img = np.pad(img, pad_width=npad, mode='constant', constant_values=0)\n",
    "            if 'raw' in f:\n",
    "                raw_images.append(img)\n",
    "            elif 'label' in f:\n",
    "                # encoded_img = one_hot_encode(img, raw_pixel_classes)\n",
    "                # segmentations.append(encoded_img)\n",
    "                segmentations.append(img)\n",
    "            filenames.append(os.path.join(folder, f))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_imgs_arr = np.array(raw_images)\n",
    "raw_imgs_arr = np.expand_dims(raw_imgs_arr, axis=3)\n",
    "\n",
    "seg_imgs_arr = np.array(segmentations)\n",
    "#seg_imgs_arr = np.expand_dims(seg_imgs_arr, axis=3)\n",
    "\n",
    "\n",
    "\n",
    "print(raw_imgs_arr.shape)\n",
    "\n",
    "print(seg_imgs_arr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cross_sec(x, model, sess):\n",
    "    prediction = model.predict(sess, x)\n",
    "    pred_classes = np.argmax(prediction[0], axis=2)\n",
    "    return pred_classes\n",
    "\n",
    "    \n",
    "def predict_whole_seg(X, model, sess):\n",
    "    '''\n",
    "    Todo: Crop the predictions. \n",
    "    '''\n",
    "    # This shape is hardcoded based on size of subject 1's scans.\n",
    "    #segmented = np.empty((X.shape[0], 482, 395))\n",
    "    segmented = np.empty(X.shape[:3])\n",
    "    print(\"shape: \", segmented.shape)\n",
    "    num_sections = X.shape[0]\n",
    "    for i in range(num_sections):\n",
    "        pred = predict_cross_sec(X[i:i+1], model, sess)\n",
    "        # segmented[i] = crop_cross_sec(pred)\n",
    "        segmented[i] = pred\n",
    "        print(i, end=', ')\n",
    "    return segmented\n",
    "\n",
    "\n",
    "def crop_cross_sec(cross_sec):\n",
    "    '''\n",
    "    512, 512 -> 482, 395\n",
    "    npad = ((15, 15), (58, 59))  # Pads to size 512, 512\n",
    "    img = np.pad(img, pad_width=npad, mode='constant', constant_values=0)\n",
    "    \n",
    "    Hardcoded right now. Todo: remember orginal dimensions and parameterize.\n",
    "    Currently all available scans have the same dimensions, so will work hardcoded for all current scans\n",
    "    as of 06/13/18\n",
    "    '''\n",
    "    return cross_sec[15:512-15,58:512-59]\n",
    "    \n",
    "\n",
    "def convert_seg_to_nifti(seg):\n",
    "    '''\n",
    "    Hardcoded right now. Todo: generalize for any scan.\n",
    "    '''\n",
    "    base_data_dir = \"/home/jessica/Documents/hart-seg-ml/allrawnifti\"\n",
    "    original_vol = nib.load(os.path.join(base_data_dir, 'trial15_60_w1_volume_TRANS.nii'))\n",
    "    new_header = original_vol.header.copy()\n",
    "    new_nifti = nib.nifti1.Nifti1Image(seg, None, header=new_header)\n",
    "    save_dir = \"/home/jessica/Documents/hart-seg-ml/predictedsegs/u-net_v1.0/30_deg_training\"\n",
    "    save_name = \"trial15_60_w1_pred_seg.nii\"\n",
    "    nib.save(new_nifti, os.path.join(save_dir, save_name))\n",
    "    \n",
    "def convert_arr_to_nifti(arrs, orig_nii_dir, trial_name, save_dir, save_name, segmented=False):\n",
    "    '''\n",
    "    arr should be tuple of numpy arrays of shape (N, height, width). Vol first then seg.\n",
    "    '''\n",
    "    orig_nii_files = {}\n",
    "    \n",
    "    for file_name in os.listdir(orig_nifti_dir):\n",
    "        if trial_name in file_name:\n",
    "            if 'volume' in file_name:\n",
    "                orig_nii_files['volume'] = file_name\n",
    "            elif 'seg' in file_name:\n",
    "                orig_nii_files['seg'] = file_name\n",
    "    \n",
    "    print(orig_nii_files)\n",
    "    \n",
    "    orig_vol_path = os.path.join(orig_nii_dir, orig_nii_files['volume'])\n",
    "    orig_seg_path = os.path.join(orig_nii_dir, orig_nii_files['seg'])\n",
    "    \n",
    "    \n",
    "    orig_vol_nii = nib.load(orig_vol_path)\n",
    "    header = orig_vol_nii.header.copy()\n",
    "    new_vol_nii = nib.nifti1.Nifti1Image(arrs[0], None, header=header)\n",
    "    save_name = trial_name + \"_proc_filled_volume.nii\"\n",
    "    nib.save(new_vol_nii, os.path.join(save_dir, save_name))\n",
    "    \n",
    "    orig_seg_nii = nib.load(orig_seg_path)\n",
    "    header = orig_seg_nii.header.copy()\n",
    "    new_seg_nii = nib.nifti1.Nifti1Image(arrs[1], None, header=header)\n",
    "    save_name = trial_name + \"_proc_filled_seg.nii\"\n",
    "    nib.save(new_seg_nii, os.path.join(save_dir, save_name))\n",
    "    \n",
    "#     for nii in orig_nii_files:\n",
    "#         if 'volume' in nii and not segmented:\n",
    "#             nii_path = os.path.join(orig_nifti_dir, nii)\n",
    "#         elif 'seg' in nii and segmented:\n",
    "#             nii_path = os.path.join(orig_nifti_dir, nii)\n",
    "            \n",
    "#     print(nii_path)\n",
    "    \n",
    "#     orig_nii = nib.load(nii_path)\n",
    "#     header = orig_nii.header.copy()\n",
    "#     new_nii = nib.nifti1.Nifti1Image(arr, None, header=header)\n",
    "    \n",
    "#     nib.save(new_nii, os.path.join(save_dir, save_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_imgs_arr.shape)\n",
    "all_segs = predict_whole_seg(raw_imgs_arr, model, sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_segs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_seg_to_nifti(all_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_shape = raw_imgs_arr.shape[:3]\n",
    "convert_seg_to_nifti(raw_imgs_arr.reshape(new_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arrs = (raw_imgs_arr, seg_imgs_arr)\n",
    "\n",
    "orig_nifti_dir = \"/home/jessica/Documents/hart-seg-ml/allrawnifti\"\n",
    "curr_trial = \"trial20_90_w1\"\n",
    "save_dir = \"/home/jessica/Documents/hart-seg-ml/allrawfillednifti\"\n",
    "# save_name = \"trial8_30_fs_proc_filled_vol.nii\"\n",
    "save_name = None\n",
    "convert_arr_to_nifti(data_arrs, orig_nifti_dir, curr_trial, save_dir, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
