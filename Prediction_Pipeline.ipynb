{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15551962006386013117\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "print(\"imported\")\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "sys.path.append('src/')\n",
    "import nn\n",
    "import process_data\n",
    "import nibabel as nib\n",
    "from math import floor, ceil\n",
    "# import cv2\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.sparse\n",
    "from scipy.misc import imrotate, imresize\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.ndimage import rotate\n",
    "from skimage import exposure\n",
    "from skimage.io import imread, imsave\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "print(local_device_protos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(L, class_labels):\n",
    "    \"\"\"\n",
    "    2D array (image) of segmentation labels -> .npy\n",
    "    # One Hot Encode the label 2d array -> .npy files with dim (h, w, len(class_labels))\n",
    "    # num classes will be 8? but currently dynamically allocated based on num colors in all scans.\n",
    "    \"\"\"\n",
    "    h, w = L.shape  # Should be 482, 395 (unless resized)\n",
    "    try:\n",
    "        encoded = np.array([list(map(class_labels.index, L.flatten()))])\n",
    "\n",
    "        L = encoded.reshape(h, w)\n",
    "\n",
    "        Lhot = np.zeros((L.shape[0], L.shape[1], len(class_labels)))\n",
    "        for i in range(L.shape[0]):\n",
    "            for j in range(L.shape[1]):\n",
    "                Lhot[i,j,L[i,j]] = 1\n",
    "        return Lhot  # Should be shape (482, 395, 9)\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "def uncode_one_hot(npy_file):\n",
    "    \"\"\"\n",
    "    .npy file -> JPEG\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def show_images(images, cols = 1, titles = None):\n",
    "    \"\"\"Display a list of images in a single figure with matplotlib.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "    cols (Default = 1): Number of columns in figure (number of rows is \n",
    "                        set to np.ceil(n_images/float(cols))).\n",
    "    \n",
    "    titles: List of titles corresponding to each image. Must have\n",
    "            the same length as titles.\n",
    "    \"\"\"\n",
    "    assert((titles is None)or (len(images) == len(titles)))\n",
    "    n_images = len(images)\n",
    "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "    fig = plt.figure()\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
    "#         if image.ndim == 2:\n",
    "#             plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "    plt.show()\n",
    "    \n",
    "def load_sparse_csr(filename):\n",
    "    # Sparse matrix reading function to read our raw .npz files\n",
    "    assert filename.endswith('.npz')\n",
    "    loader = np.load(filename)  # filename must end with .npz\n",
    "    return scipy.sparse.csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])\n",
    "\n",
    "def get_raw_pixel_classes(trial_name, raw_nifti_dir):\n",
    "    trial_segmentation = None\n",
    "    \n",
    "    for raw_nii in os.listdir(raw_nii_dir):\n",
    "        if raw_nii.startswith(trial_name) and raw_nii.endswith(\".nii\"):\n",
    "            if \"seg\" in raw_nii:\n",
    "                trial_segmentation = os.path.join(raw_nii_dir, raw_nii)\n",
    "                break\n",
    "    \n",
    "    scan_voxel = nib.load(trial_segmentation)\n",
    "    struct_arr = scan_voxel.get_data()\n",
    "    class_labels = sorted(list(np.unique(struct_arr)))\n",
    "    return class_labels\n",
    "    \n",
    "def check_one_hot(encoded_img):\n",
    "    print(encoded_img.shape)\n",
    "    return np.all(np.sum(encoded_img, axis=2) == 1.)\n",
    "\n",
    "def batch_img_resize(images, h = 256, w = 256):\n",
    "    images_resized = np.zeros([0, newHeight, newWidth], dtype=np.uint8)\n",
    "    for image in range(images.shape[0]):\n",
    "        temp = imresize(images[image], [h, w], 'nearest')\n",
    "        images_resized = np.append(images_resized, np.expand_dims(temp, axis=0), axis=0)\n",
    "    return images_resized\n",
    "\n",
    "def pad_image(orig_img, height, width):    \n",
    "    orig_height, orig_width = orig_img.shape\n",
    "    \n",
    "    height_pad = (height - orig_height) / 2\n",
    "    width_pad = (width - orig_width) / 2\n",
    "    \n",
    "    height_top_pad = floor(height_pad)\n",
    "    height_bot_pad =  ceil(height_pad)\n",
    "    width_left_pad = floor(width_pad)\n",
    "    width_right_pad = ceil(width_pad)\n",
    "    \n",
    "    pad_dims = ((height_top_pad, height_bot_pad), (width_left_pad, width_right_pad))\n",
    "    padded_img = np.pad(orig_img, pad_width=pad_dims, mode='constant', constant_values=0)\n",
    "    \n",
    "    return padded_img\n",
    "\n",
    "def load_data(processed_data_dir, raw_data_dir, height=512, width=512, use_default_classes=True):\n",
    "    \"\"\"\n",
    "    Load both the raw, unlabeled scan data as well as corresponding labeled segmentation data.\n",
    "    Can draw from an arbitrary number of scan/segmentation pairs. All raw data ends up in the\n",
    "    same array, all segmentation data ends up in the same array.\n",
    "    \n",
    "    Args:\n",
    "        processed_data_dir (str): Path to directory containing separate folders for the preprocessed\n",
    "            data, where each folder contains all .npz and .png files for one scan. Name expected\n",
    "            to resemble, e.g. \"trial8_30_fs\".\n",
    "        raw_data_dir (str): Path to directory containing original .nii files of each scan being used,\n",
    "            each should not be in sub-folder. Should include both unsegmented and segmented .nii.\n",
    "        height (int): Height in pixels to which scan cross sections get resized.\n",
    "        width (int): Width in pixels to which scan cross sections get resized.\n",
    "        use_default_classes (bool): Flag indicating whether each scan being used should use default\n",
    "            set of classes. If false, classes will be individually determined for each scan.\n",
    "            \n",
    "    Returns:\n",
    "        tuple: Tuple of numpy arrays where the first array contains the raw data and is of shape\n",
    "            (N, height, width), where N is total number of cross sections from all scans; the second\n",
    "            contains the segmented data and is of shape (N, 512, 512, C) where C is the number of\n",
    "            pixel classes. By default C is 9. Class dimension is one-hot-encoded. \n",
    "    \"\"\"\n",
    "    default_raw_pixel_classes = [0, 7, 8, 9, 45, 51, 52, 53, 68]\n",
    "    raw_images = []\n",
    "    segmentations = []\n",
    "    all_raw_pixel_classes = {}\n",
    "    \n",
    "    for folder in os.listdir(processed_data_dir):\n",
    "        if not folder.startswith('.'):\n",
    "            trial_folder_path = os.path.join(processed_data_dir, folder)\n",
    "            print(\"====\")\n",
    "            print(trial_folder_path)\n",
    "            print(\"====\")\n",
    "\n",
    "            trial_files = []\n",
    "\n",
    "            for item in os.listdir(trial_folder_path):\n",
    "                item_path = os.path.join(trial_folder_path, item)\n",
    "                if os.path.isfile(item_path) and not item.startswith('.'):\n",
    "                    trial_files.append(item)\n",
    "\n",
    "            trial_files = sorted(trial_files)\n",
    "\n",
    "            \n",
    "            if use_default_classes:\n",
    "                all_raw_pixel_classes[folder] = default_raw_pixel_classes\n",
    "            else:\n",
    "                all_raw_pixel_classes[folder] = get_raw_pixel_classes(folder, raw_data_dir)\n",
    "\n",
    "            for file in trial_files:\n",
    "                print(file, end=\", \")\n",
    "                if 'label' in file:\n",
    "                    img = imread(os.path.join(trial_folder_path, file), flatten=True)\n",
    "                else:\n",
    "                    img = load_sparse_csr(os.path.join(trial_folder_path, file)).toarray()            \n",
    "\n",
    "                img = pad_image(img, height, width)\n",
    "                if 'raw' in file:\n",
    "                    raw_images.append(img)\n",
    "                elif 'label' in file:\n",
    "                    encoded_img = one_hot_encode(img, all_raw_pixel_classes[folder])\n",
    "                    segmentations.append(encoded_img)\n",
    "        print(\"\")\n",
    "    \n",
    "    raw_images_arr = np.array(raw_images)\n",
    "    segmentations_arr = np.array(segmentations)\n",
    "    return raw_images_arr, segmentations_arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====\n",
      "/Users/nozik/Documents/HARTresearch/unet/hart-seg-ml/pipeline_test/trial12_30_w3\n",
      "====\n",
      "0_label.png, 0_raw.npz, 10_label.png, 10_raw.npz, 11_label.png, 11_raw.npz, 12_label.png, 12_raw.npz, 13_label.png, 13_raw.npz, 14_label.png, 14_raw.npz, 15_label.png, 15_raw.npz, 16_label.png, 16_raw.npz, 17_label.png, 17_raw.npz, 18_label.png, 18_raw.npz, 19_label.png, 19_raw.npz, 1_label.png, 1_raw.npz, 20_label.png, 20_raw.npz, 21_label.png, 21_raw.npz, 22_label.png, 22_raw.npz, 23_label.png, 23_raw.npz, 24_label.png, 24_raw.npz, 25_label.png, 25_raw.npz, 26_label.png, 26_raw.npz, 27_label.png, 27_raw.npz, 28_label.png, 28_raw.npz, 29_label.png, 29_raw.npz, 2_label.png, 2_raw.npz, 30_label.png, 30_raw.npz, 3_label.png, 3_raw.npz, 4_label.png, 4_raw.npz, 5_label.png, 5_raw.npz, 6_label.png, 6_raw.npz, 7_label.png, 7_raw.npz, 8_label.png, 8_raw.npz, 9_label.png, 9_raw.npz, \n",
      "====\n",
      "/Users/nozik/Documents/HARTresearch/unet/hart-seg-ml/pipeline_test/trial15_60_w1\n",
      "====\n",
      "0_label.png, 0_raw.npz, 10_label.png, 10_raw.npz, 11_label.png, 11_raw.npz, 12_label.png, 12_raw.npz, 13_label.png, 13_raw.npz, 14_label.png, 14_raw.npz, 15_label.png, 15_raw.npz, 16_label.png, 16_raw.npz, 17_label.png, 17_raw.npz, 18_label.png, 18_raw.npz, 19_label.png, 19_raw.npz, 1_label.png, 1_raw.npz, 20_label.png, 20_raw.npz, 21_label.png, 21_raw.npz, 22_label.png, 22_raw.npz, 23_label.png, 23_raw.npz, 24_label.png, 24_raw.npz, 25_label.png, 25_raw.npz, 26_label.png, 26_raw.npz, 27_label.png, 27_raw.npz, 28_label.png, 28_raw.npz, 29_label.png, 29_raw.npz, 2_label.png, 2_raw.npz, 30_label.png, 30_raw.npz, 3_label.png, 3_raw.npz, 4_label.png, 4_raw.npz, 5_label.png, 5_raw.npz, 6_label.png, 6_raw.npz, 7_label.png, 7_raw.npz, 8_label.png, 8_raw.npz, 9_label.png, 9_raw.npz, \n"
     ]
    }
   ],
   "source": [
    "raw_nii_dir = \"/Users/nozik/Documents/HARTresearch/allrawnifti\"\n",
    "preproc_data_dir = \"/Users/nozik/Documents/HARTresearch/unet/hart-seg-ml/pipeline_test\"\n",
    "\n",
    "raw_data, seg_data = load_data(preproc_data_dir, raw_nii_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 512, 512)\n",
      "(62, 512, 512, 9)\n"
     ]
    }
   ],
   "source": [
    "print(raw_data.shape)\n",
    "print(seg_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
