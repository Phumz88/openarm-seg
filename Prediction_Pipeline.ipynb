{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9588057621001914050\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "print(\"imported\")\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "sys.path.append('src/')\n",
    "import nn\n",
    "import process_data\n",
    "import nibabel as nib\n",
    "from math import floor, ceil\n",
    "# import cv2\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.sparse\n",
    "from scipy.misc import imrotate, imresize\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.ndimage import rotate\n",
    "from skimage import exposure\n",
    "from skimage.io import imread, imsave\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "print(local_device_protos)\n",
    "\n",
    "import pipeline\n",
    "import Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/nozik/Documents/HARTresearch/unet/hart-seg-ml/pipeline_test/trial12_30_w3', '/Users/nozik/Documents/HARTresearch/unet/hart-seg-ml/pipeline_test/trial15_60_w1']\n",
      "====\n",
      "/Users/nozik/Documents/HARTresearch/unet/hart-seg-ml/pipeline_test/trial12_30_w3\n",
      "====\n",
      "0_label.png 0_raw.npz 10_label.png 10_raw.npz 11_label.png 11_raw.npz 12_label.png 12_raw.npz 13_label.png 13_raw.npz 14_label.png 14_raw.npz 15_label.png 15_raw.npz 16_label.png 16_raw.npz 17_label.png 17_raw.npz 18_label.png 18_raw.npz 19_label.png 19_raw.npz 1_label.png 1_raw.npz 20_label.png 20_raw.npz 21_label.png 21_raw.npz 22_label.png 22_raw.npz 23_label.png 23_raw.npz 24_label.png 24_raw.npz 25_label.png 25_raw.npz 26_label.png 26_raw.npz 27_label.png 27_raw.npz 28_label.png 28_raw.npz 29_label.png 29_raw.npz 2_label.png 2_raw.npz 30_label.png 30_raw.npz 3_label.png 3_raw.npz 4_label.png 4_raw.npz 5_label.png 5_raw.npz 6_label.png 6_raw.npz 7_label.png 7_raw.npz 8_label.png 8_raw.npz 9_label.png 9_raw.npz ====\n",
      "/Users/nozik/Documents/HARTresearch/unet/hart-seg-ml/pipeline_test/trial15_60_w1\n",
      "====\n",
      "0_label.png 0_raw.npz 10_label.png 10_raw.npz 11_label.png 11_raw.npz 12_label.png 12_raw.npz 13_label.png 13_raw.npz 14_label.png 14_raw.npz 15_label.png 15_raw.npz 16_label.png 16_raw.npz 17_label.png 17_raw.npz 18_label.png 18_raw.npz 19_label.png 19_raw.npz 1_label.png 1_raw.npz 20_label.png 20_raw.npz 21_label.png 21_raw.npz 22_label.png 22_raw.npz 23_label.png 23_raw.npz 24_label.png 24_raw.npz 25_label.png 25_raw.npz 26_label.png 26_raw.npz 27_label.png 27_raw.npz 28_label.png 28_raw.npz 29_label.png 29_raw.npz 2_label.png 2_raw.npz 30_label.png 30_raw.npz 3_label.png 3_raw.npz 4_label.png 4_raw.npz 5_label.png 5_raw.npz 6_label.png 6_raw.npz 7_label.png 7_raw.npz 8_label.png 8_raw.npz 9_label.png 9_raw.npz "
     ]
    }
   ],
   "source": [
    "test_trial_dir = \"/Users/nozik/Documents/HARTresearch/unet/hart-seg-ml/pipeline_test/\"\n",
    "raw, seg = pipeline.load_all_data(test_trial_dir, encode_segs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_arr = np.asarray(raw)\n",
    "seg_arr = np.asarray(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 512, 512, 1)\n",
      "(19, 512, 512, 1)\n",
      "(6, 512, 512, 1)\n",
      "(37, 512, 512, 9)\n",
      "(19, 512, 512, 9)\n",
      "(6, 512, 512, 9)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, x_test, y_train, y_val, y_test = pipeline.split_data(raw, seg, 60, 10, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/nozik/Documents/HARTresearch/unet/hart-seg-ml/Unet.py:17: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "mean = 0\n",
    "weight_decay = 1e-6\n",
    "learning_rate = 1e-4\n",
    "label_dim = 8\n",
    "\n",
    "# Create TF graph and initialize variables\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "model = Unet.Unet(mean, weight_decay, learning_rate, label_dim, dropout = 0.5)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = '/Users/nozik/Documents/HARTresearch/unet/hart-seg-ml/models'\n",
    "model_name = 'test_june18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "/home/jessica/Documents/hart-seg-ml/models/test_june18; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-417234cadd50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/HARTresearch/unet/hart-seg-ml/pipeline.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(models_dir, model_name, saver, sess)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0mmeta_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mlatest_checkpoint\u001b[0;34m(checkpoint_dir, latest_filename)\u001b[0m\n\u001b[1;32m   1740\u001b[0m     v1_path = _prefix_to_checkpoint_path(ckpt.model_checkpoint_path,\n\u001b[1;32m   1741\u001b[0m                                          saver_pb2.SaverDef.V1)\n\u001b[0;32m-> 1742\u001b[0;31m     if file_io.get_matching_files(v2_path) or file_io.get_matching_files(\n\u001b[0m\u001b[1;32m   1743\u001b[0m         v1_path):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0;31m# Convert the filenames to string from bytes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0msingle_filename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m           for matching_filename in pywrap_tensorflow.GetMatchingFiles(\n\u001b[1;32m    335\u001b[0m               compat.as_bytes(single_filename), status)\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: /home/jessica/Documents/hart-seg-ml/models/test_june18; No such file or directory"
     ]
    }
   ],
   "source": [
    "pipeline.load_model(models_dir, model_name, saver, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
