{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16774242041956609891\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11970700903\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 129995666667014192\n",
      "physical_device_desc: \"device: 0, name: TITAN Xp, pci bus id: 0000:02:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# %load_ext autoreload\n",
    "# %autoreload 1 # Always reload before execution only modules imported with '%aimport'\n",
    "# %aimport pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "sys.path.append('src/')\n",
    "import nn\n",
    "import process_data\n",
    "import nibabel as nib\n",
    "from math import floor, ceil\n",
    "from importlib import reload\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.sparse\n",
    "from scipy.misc import imrotate, imresize\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.ndimage import rotate\n",
    "from skimage import exposure\n",
    "from skimage.io import imread, imsave\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "print(local_device_protos)\n",
    "\n",
    "import pipeline\n",
    "import Unet\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('__name__')\n",
    "stream = logging.StreamHandler(stream=sys.stdout)\n",
    "stream.setFormatter(logging.Formatter(\"%(levelname)-8s %(message)s\"))\n",
    "logger.handlers = []\n",
    "logger.addHandler(stream)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG    Debug log from notebook.\n",
      "INFO     Info log from notebook.\n",
      "WARNING  Warning log from notebook.\n",
      "ERROR    Error log from notebook.\n",
      "CRITICAL Critical log from notebook.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pipeline' has no attribute 'log_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a9d8f6869cf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtestvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pipeline' has no attribute 'log_test'"
     ]
    }
   ],
   "source": [
    "logger.debug(\"Debug log from notebook.\")\n",
    "logger.info(\"Info log from notebook.\")\n",
    "logger.warning(\"Warning log from notebook.\")\n",
    "logger.error(\"Error log from notebook.\")\n",
    "logger.critical(\"Critical log from notebook.\")\n",
    "\n",
    "testvar = 5\n",
    "pipeline.log_test(testvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant to be a demo and documentation for using the models and generating predictions. Feel free to make copies for your own use but please do not push your changes to it (unless there's an error, in which case submit a pull request). \n",
    "\n",
    "While this notebook will show you how to generate predictions on your own computer, unless you have very good hardware it will be very slow. On my computer (2.7 GHz Intel Core i5, 8 GB RAM, Intel Iris Graphics 6100 1536 MB) generating a prediction for a single cross section takes about 15 seconds and full scans have at minimum around 600 cross sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Tensorflow session and a dummy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This needs to be done before any operation involving training or generating segmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "model = Unet.Unet(0, 0.5, 0.5) # Arbitrary initialization\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this you need to specify the directory where the models are held as well as the saved model name, which should match both the name of the subfolder that holds the model as well as the `[model_name].data, [model_name].meta`, and `[model_name].index` files. The data file might have something on the end of its extension, like `[model_name].data-00000-of-00001`. That's fine.\n",
    "\n",
    "Note that `models_dir` should not be the highest-level directory containing folders for each model architecture, e.g. `/media/jessica/Storage/models/`, but should be one of the folders corresponding to a particular model architecture that in turn holds different trained versions. In kind-of picture form:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "models \n",
    "|-- u-net_v1-0 \n",
    "|   |-- pretrained_version_1\n",
    "|   |   |-- checkpoint\n",
    "|   |   |-- pretrained_version_1.data\n",
    "|   |   |-- pretrained_version_1.index\n",
    "|   |   `-- pretrained_version_1.meta\n",
    "|   |-- pretrained_version_2\n",
    "|   `-- ...\n",
    "|-- u-net_v2-0\n",
    "|   |-- pretrained_version_1\n",
    "|   |-- pretrained_version_2\n",
    "|   `-- ...\n",
    "`-- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above picture, don't use `models` as `models_dir`. You could use the absolute path to `u-net_v1-0` or `u-net_v2-0` and then the model name (same as folder name) of one of the pretrained versions, e.g. `pretrained_version_1`.\n",
    "\n",
    "The below example will work on the main office machine but won't on your own unless you download the model and put it on your machine and set the directories right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = '/media/jessica/Storage/models/u-net_v1-0'\n",
    "model_name = '30_deg_training_sorted_inputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.load_model(models_dir, model_name, saver, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict many segmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best function for generating segmentations is the `predict_all_segs` function in `pipeline.py`. With this function you can easily generate predictions for every single preprocessed scan and NIfTI pair you have (or in general as many as you want). To use this, the Tensorflow session and model need to be loaded as above and then three directories need to be specified: the directory containing separate folders each holding preprocessed scan data, the directory where all the resulting segmentations will be placed, and the directory holding the original `.nii` files. Note for this last directory that you will most likely want to use `/media/jessica/Storage/allrawfillednifti` rather than `/media/jessica/Storage/allrawfnifti`. The difference is the former holds the reconstructed `.nii` files where the deltoid is filled in. Additionally, the scan dimensions will match the generated predictions, so you can open the predictions over these filled `.nii` files. Once these are specified, all you need to do is call the function with the directories as well the Tensorflow model and session arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_segmented_dir = \"/media/jessica/Storage/preprocesseddata\"\n",
    "new_segs_location_dir = \"/media/jessica/Storage/predictedsegs/u-net_v1-0/30_deg_training_sorted_inputs/restored_labels\"\n",
    "orig_nii_dir = \"/media/jessica/Storage/allrawfillednifti\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict_all_segs(to_be_segmented_dir, new_segs_location_dir, orig_nii_dir, model, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict a single segmentation (or control intermediate steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general `predict_all_segs` is the best way to make segmentations, even if you just want to generate one segmentation. If you're manually loading and modifying data or otherwise want access to intermediate steps then you can do that as well, but it will be more inconvenient, especially if you want to do this for more than one scan.\n",
    "\n",
    "To load the data from a single scan, use `load_data` from `pipeline.py`. You will need to provide the path to the directory which directly holds the `.npz` and `.png` files of the scan you want to segment. Note you can set the optional flag `encode_segs` (True by default) to determine whether you want to one-hot-encode the ground truth labels. If you are just generating a segmentation this should probably be set to False because encoding can take a few minutes. If you have no label files it doesn't matter. Note the outputs from this function are lists whereas future functions expect Numpy arrays, so you will have to convert them before passing them to prediction functions. It's likely best for now to leave the `height` and `width` arguments as their default values to ensure compatibility with existing scans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir = \"/media/jessica/Storage/preprocesseddata/trial8_30_fs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_imgs_lst, seg_imgs_lst = pipeline.load_data(processed_data_dir, encode_segs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See output details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(raw_imgs_lst), len(seg_imgs_lst), sep=', ')\n",
    "print(type(raw_imgs_lst), type(seg_imgs_lst), sep=', ')\n",
    "print(type(raw_imgs_lst[0]), type(seg_imgs_lst[0]), sep=', ')\n",
    "print(raw_imgs_lst[0].shape, seg_imgs_lst[0].shape, sep=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having loaded the data, you can proceed to do whatever inspection, visualization, or processing you want. Once you want to predict, though, you will need to convert the lists to Numpy arrays. You can then pass the array into `predict_whole_seg`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to array and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_imgs_arr = np.asarray(raw_imgs_lst)\n",
    "print(\"Shape of new array:\", raw_imgs_arr.shape)\n",
    "predicted_seg = pipeline.predict_whole_seg(raw_imgs_arr, model, sess)\n",
    "print(\"Shape of resulting array (should be same as before):\", predicted_seg.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something to note is that the prediction functions actually want arrays to have a channel dimension (which for our purposes is of size 1; for RGB it would be 3). This is handled in the prediction function (see the call to `np.expand_dims`) but it might be important to know if you start directly calling some of those functions, like `predict_image` (which doesn't check this)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the segmentation you can play with it some more or save it as a NIfTI file by calling `save_arr_as_nifti`. Since you're calling it manually you will need to explicitly provide the name of the original NIfTI file (including the file extension) as well as the name to save the NIfTI as (also including file extension). You will also need the path to the directory where the NIfTI is stored and the path to the directory where you want to save the NIfTI. Again, note that the original NIfTI files are the filled versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_nifti_name = \"trial8_30_fs_proc_filled_volume.nii\"\n",
    "new_nifti_name = \"demo.nii\"\n",
    "orig_nii_dir = \"/media/jessica/Storage/allrawfillednifti\"\n",
    "save_dir = \"/media/jessica/Storage/demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predicted segmentation as NIfTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.save_arr_as_nifti(predicted_seg, orig_nifti_name, new_nifti_name, orig_nii_dir, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
